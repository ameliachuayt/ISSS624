[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "",
    "text": "There are two parts in this hands-on exercise:"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#overview-of-geospatial-data-wrangling",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#overview-of-geospatial-data-wrangling",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.1 Overview of Geospatial Data Wrangling",
    "text": "1.1 Overview of Geospatial Data Wrangling\nIn the first of this two-part hands-on exercise, I learned how to import, and wrangle geospatial data using appropriate R packages."
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#data-acquisition",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#data-acquisition",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.2 Data Acquisition",
    "text": "1.2 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. For this hands-on exercise, data sets were extracted from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.3 Getting Started",
    "text": "1.3 Getting Started\nThe code chunk below installs and loads sf and tidyverse packages into R environment. pacman() is a R package management tool. It provides intuitively named functions for the base functions.\n\npacman::p_load(sf, tidyverse)\n\nAn alternate way to install and import the libraries is as follows:\n\npackages = c('sf','tidyverse')\nfor (p in packages){\n  if(!require(p, character.only = T)){\n    install.packages(p)\n  }\n  library(p, character.only = T)\n}"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#import-geospatial-data",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#import-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.4 Import Geospatial Data",
    "text": "1.4 Import Geospatial Data\nIn this section, I learned how to import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n1.4.1 Import polygon feature date in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame.\nWhen the input geospatial data is in shapefile format (.shp), two arguments will be used: dsn to define the data path and layer to provide the shapefile name. No extensions such as .shp, .dbf, .prj and .shx are reqquired.\n\nmpsz = st_read(dsn=\"data\\\\geospatial\",\n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe output shows that there are 323 multipolygon features and 15 fields. The Bounding box provides the x extend and y extend of the data.\n\n\n1.4.2 Import polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as a line feature data frame.\n\ncyclingpath = st_read(dsn = \"data\\\\geospatial\",\n                      layer = 'CyclingPath')\n\nReading layer `CyclingPath' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe output shows that there are 1625 features and 2 fields in CyclingPath linestring feature data frame and it is in svy21 projected coordinates system.\n\n\n1.4.3 Import GIS data in kml format\nThe code chunk below uses st_read() function of sf package to import pre-schools-location-kml kml file into R as a point feature layer. Since we are dealing with a kml file, instead of specifying dsn and layer, we specify the complete path and file extension.\n\npreschool = st_read(\"data\\\\geospatial\\\\pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\Hands-on_Ex1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe output reveals that preschool is a point feature data frame (see “Geometry type: POINT’”). There are a total of 1359 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system (see “Geodetic CRS: WGS 84”)."
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.5 Checking the Content of A Simple Feature Data Frame",
    "text": "1.5 Checking the Content of A Simple Feature Data Frame\nIn the section, I learned different ways to retrieve information related to the contents of a simple feature data frame.\n\n1.5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nThe output displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n1.5.2 Working with glimpse().\nTo learn more about the associated attribute information in the data frame, we can use glimpse() of dplyr.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nAs we can see in the output, glimpse() reveals the data type of each field.\nFor example, FMEL-UPD_D is in date data ‘<date>’, and X_ADDR, Y_ADDR are in double precision values ‘<dbl>’\n\n\n1.5.3 Working with head()\nSometimes, we would like to examine complete information of a feature object. We can use head() of Base R to achieve this. The argument n allows us to indicate the number of records to display.\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.6 Plotting the Geospatial Data",
    "text": "1.6 Plotting the Geospatial Data\nIn geospatial analytics, we are definitely interested to visualise geospatial features. plot() provides a quick and simply way to visualise the data at hand.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\nThe default plot of sf object is a multi-plot of all attributes, up to a maximum limit (in this case, it is 9 out of 15) as shown above.\nWe can also choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\nWe can also choose to plot the sf object by using a specific attribute.\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\nAs mentioned earlier, plot() is meant for plotting the geospatial object for quick look. For high cartographic quality plot with more customisation options, tmap or other packages should be used."
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.7 Working with Projection",
    "text": "1.7 Working with Projection\nIn order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system. Projection Transformation is the process of projecting a simple feature data frame from one coordinate system to another coordinate system.\n\n1.7.1 Assigning EPSG code to a simple feature data frame\nEPSG stands for European Petroleum Survey Group and is an organisation that maintains a public registry of geodetic parameter database with standard codes–the EPSG codes.\nOne common issue faced during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nWe can examine the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough the mpsz data frame is projected in ‘svy21’, when we read till the end of the print it indicates that EPSG is 9001, which is wrong. See the last line where “ID[”EPSG”,9001]“.\nThe correct/corresponding EPSG code for ‘svy21’ should be ‘3414’. We can assign the correct EPSG code using st_set_crs() of sf package.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nTo confirm the change, we can check the coordinate system or CSR again.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNote that the EPSG code is 3414. See last line where “ID[”EPSG”,3414]“.\n\n\n1.7.2 Transforming the projection of preschool from wgs84 to svy21\nIn this sub-section, we will learn how to transform original data from geographic coordinate system to projected coordinate system. We need to do this transformation because the geographic coordinate system is inappropriate if the analysis require the use of distance and/or area measurements.\nFor the preschool simple feature data frame, the output of the code chunk below tells us that it is in the wgs84 coordinate system (see “Geodetic CRS: WGS 84”).\n\nst_geometry(preschool)\n\nGeometry set for 1359 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.7614 1.308683 0)\n\n\nPOINT Z (103.7536 1.315748 0)\n\n\nPOINT Z (103.7645 1.305078 0)\n\n\nPOINT Z (103.765 1.305239 0)\n\n\nPOINT Z (103.7597 1.315983 0)\n\n\nInstead of using st_set_crs() like we did in the previous section, we must use st_transform() of the sf package. This is because we need to reproject preschool from one coordinate system to another coordinate system mathematically.\n\npreschool3414 <- st_transform(preschool,\n                              crs = 3414)\n\n\nNote: In practice, we need to find out the appropriate project coordinate system to use before performing the projection transformation.\n\nLet’s check if the transformation is complete.\n\nst_geometry(preschool3414)\n\nGeometry set for 1359 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25667.6 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nPOINT Z (19997.26 32333.17 0)\n\n\nPOINT Z (19126.75 33114.35 0)\n\n\nPOINT Z (20345.12 31934.56 0)\n\n\nPOINT Z (20400.31 31952.36 0)\n\n\nPOINT Z (19810.78 33140.31 0)\n\n\nFrom the output, we can see that it is in the svy21 projected coordinate system now (see “Projected CRS: SVY21 / Singapore TM). Also, the Bounding box values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems.\nAlternatively, the code chunk below will import and transform GIS data into projected coordinates system data. (Instead of importing and transforming in two separate code chunks.)\n\npreschool = st_read(\"data\\\\geospatial\\\\pre-schools-location-kml.kml\") %>%\n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\Hands-on_Ex1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nWhile the results states that Geodetic CRS is WGS84, we can examine the dataframe and see that under geometry , the coordinates are no longer in wgs84 (the values are larger than 360)."
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#importing-and-converting-aspatial-data",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#importing-and-converting-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.8 Importing and Converting Aspatial Data",
    "text": "1.8 Importing and Converting Aspatial Data\nIt is common to have data such as listing of inside Airbnb. Such data are called aspatial data. They are not geospatial data, however, among the data fields, there are two fields that capture the x- and y- coordinates of the data points.\nIn this section, I learned to import aspatial data in R environment and save it as a tibbledata frame. Then, I will convert it into a simple feature data frame.\n\n1.8.1 Importing aspatial data\nlistings.csv data set is in csv format and we will use read_csv() of readr package to import the file. The output R object is called listings and is a tibble data frame.\n\nlistings <- read_csv('data\\\\aspatial\\\\listings.csv')\n\nRows: 4252 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (5): name, host_name, neighbourhood_group, neighbourhood, room_type\ndbl  (10): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s examine if the import was completely correctly.\n\nlist(listings)\n\n[[1]]\n# A tibble: 4,252 × 16\n       id name     host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n    <dbl> <chr>      <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1  50646 Pleasan…  227796 Sujatha Centra… Bukit …    1.33    104. Privat…    80\n 2  71609 Ensuite…  367042 Belinda East R… Tampin…    1.35    104. Privat…   178\n 3  71896 B&B  Ro…  367042 Belinda East R… Tampin…    1.35    104. Privat…    81\n 4  71903 Room 2-…  367042 Belinda East R… Tampin…    1.35    104. Privat…    81\n 5 275343 Conveni… 1439258 Joyce   Centra… Bukit …    1.29    104. Privat…    52\n 6 275344 15 mins… 1439258 Joyce   Centra… Bukit …    1.29    104. Privat…    40\n 7 294281 5 mins … 1521514 Elizab… Centra… Newton     1.31    104. Privat…    72\n 8 301247 Nice ro… 1552002 Rahul   Centra… Geylang    1.32    104. Privat…    41\n 9 324945 20 Mins… 1439258 Joyce   Centra… Bukit …    1.29    104. Privat…    49\n10 330089 Accomo@… 1439258 Joyce   Centra… Bukit …    1.29    104. Privat…    49\n# … with 4,242 more rows, 6 more variables: minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>, and\n#   abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System\nNote that list() instead of glimpse() was used above. In the code chunk below, we can also print the features of the data using glimpse(). In glimpse(), the columns run down the page and data runs across, enabling us to see all the columns easily.\n\nglimpse(listings)\n\nRows: 4,252\nColumns: 16\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275343, 275…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ latitude                       <dbl> 1.33432, 1.34537, 1.34754, 1.34531, 1.2…\n$ longitude                      <dbl> 103.7852, 103.9589, 103.9596, 103.9610,…\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, 41, 49, 49…\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8, 14, 14, …\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, 105, 14, 1…\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014-12-10, 20…\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20, 0.16, 1.2…\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50, 50, 50, 4…\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364, 365, 90, …\n\n\n\n\n1.8.2 Create simple feature dataframe from aspatial dataframe\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nlistings_sf <- st_as_sf(listings,\n                        coords = c(\"longitude\",\"latitude\"), #x-coord first, then y-coord\n                        crs=4326) %>% #provide coordinates system in epsg format\n                                      #EPSG:4326 is wgs84\n                                      #EPSG:3414 is Singapore's SVY21 Projected Coordinate System\n  st_transform(crs = 3414)\n\nSeveral things to take note of from the arguments above:\n\ncoords argument requires us to input column name of x-coordinate first, followed by column name of y-coordinate\ncrs argument requires us to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. We can search for other country’s epsg code by referring to epsg.io.\n%>% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet’s examine the content of the newly created simple feature data frame. Note that there is a new column geometry that has been added. Also, the longitude and latitude columns have been dropped.\n\nglimpse(listings_sf)\n\nRows: 4,252\nColumns: 15\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275343, 275…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, 41, 49, 49…\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8, 14, 14, …\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, 105, 14, 1…\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014-12-10, 20…\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20, 0.16, 1.2…\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50, 50, 50, 4…\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364, 365, 90, …\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#geo-processing-with-sf-package",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#geo-processing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.9 Geo-processing with sf package",
    "text": "1.9 Geo-processing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, I learned how to perform two commonly used geoprocessing functions:\n\nbuffering and\npoint in polygon count\n\n\n1.9.1 Buffering\nBuffering involves measuring the distance outward in all directions from an object. The output is a polygon.\nTo illustrate how buffering works, how is a hypothetical scenario:\n\n\n\n\n\n\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path.\nTask: Determine the extent of the land that needs to be acquired and their total area.\n\n\n\nSteps to solve:\nStep 1: Compute the 5-meter buffers around the cycling paths\n\nbuffer_cycling <- st_buffer(cyclingpath,\n                            dist = 5, #5 metres\n                            nQuadSegs = 30)\n\nStep 2: Calculate area of buffers\nAs mentioned earlier, the output of buffering is polygons. So here, we can create a new column AREA to store the values of the areas of polygons\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nStep 3: Derive Total Land Area\nTo do this, we can easily use sum() of Base R\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\n\n\n1.9.2 Point-in-polygon Count\nWe can also count the frequency of observations within a polygon. To illustrate this, we have another hypothetical situation\n\n\n\n\n\n\nNote\n\n\n\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nTask: Find out the numbers of pre-schools in each Planning Subzone and the density per square metres.\n\n\nStep 1: Identify pre-schools located in each Subzone and Calculate number of pre-schools in each subzone\nWe can use st_interesects() to identify which subzones pre-schools are located in and lengths() to count the number of pre-schools that fall inside each subzone.\n\nmpsz3414$`PreSch Count` <- lengths(st_intersects(mpsz3414, preschool3414))\n\nCheck descriptive statistics using the below code chunk.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\n\nWe can also list the subzone with the most pre schools using top_n() of dplyr package. We can change the argument within top_n() according to requirements e.g., Top 3, 5, or 10, etc.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   6603.608    2553464 MULTIPOLYGON (((24786.75 46...           37\n\n\nStep 2: Derive area of each subzone\nThe code chunk below uses st_area() of sf package to derive the area of each subzone. We are creating a new column Area to store the area values.\n\nmpsz3414$Area <- mpsz3414 %>% \n  st_area()\n\nStep 3: Calculate Density\nWe can simply calculate the density by using mutate() of dplyr package. A new column PreSch Density is created.\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "1.10 Exploratory Data Analysis (EDA)",
    "text": "1.10 Exploratory Data Analysis (EDA)\nIn this section, I learned appropriate ggplot2 functions to create function yet truthful statistical graphs for EDA purposes.\n\n1.10.0.1 Distribution of Pre-school Density in Subzones of Singapore using Histograms\nConventionally, hist() of R Graphics can be used to plot a histogram of the distribution of pre-school density. While hist()’s syntax is easy to use, the output does not meet publication quality and it has limited room for customisation.\n\nhist(mpsz3414$`PreSch Density`) \n\n\n\n\nLet’s retry to ggplot2 functions instead.\n\nggplot(data=mpsz3414,\n       aes(x=as.numeric(`PreSch Density`))) +\n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill = \"light blue\") + \n  labs(title = \"Are pre-schools evenly distributed in Singapore?\",\n       subtitle = \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n       x = \"Pre-school density (per km sq)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n1.10.0.2 Relationship between Pre-school Density and Pre-school Count using Scatterplot\nDIY: Conventionally, plot() of R Graphics can be used to plot a scatterplot to reveal the relationship between pre-school density and pre-school count.\n\nplot(mpsz3414$`PreSch Density`, mpsz3414$`PreSch Count`, main=\"Pre-school Count vs Pre-school Density\",\n    pch=19)\n\n\n\n\nHowever, we may also opt to use ggplot2 for it has better customisation capabilities.\n\nlibrary(units)\n\nudunits database from C:/R/R-4.2.2/library/units/share/udunits/udunits2.xml\n\nggplot(data=mpsz3414, aes(x=`PreSch Density`, y=`PreSch Count`))+\n  geom_point()+ \n  labs(x = \"Pre-school density (per km sq)\",\n       y = \"Pre-school count\")"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#overview-of-choropleth-mapping-with-r",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#overview-of-choropleth-mapping-with-r",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "2.1 Overview of Choropleth Mapping with R",
    "text": "2.1 Overview of Choropleth Mapping with R\nIn the second of this two-part hands-on exercise, I learned how to plot functional and truthful choropleth maps by using an R package called tmap package.\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#data-acquisition-1",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#data-acquisition-1",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "2.2 Data Acquisition",
    "text": "2.2 Data Acquisition\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile."
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#getting-started-importing-data",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#getting-started-importing-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "2.3 Getting Started & Importing Data",
    "text": "2.3 Getting Started & Importing Data\n\n2.3.1 Getting Started\nThe key R package for this hands-on exercise is tmap package in R. We will also be using four other R packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data\n\nThree out of the four are packages (readr, tidyr and dplyr) are part of the tidyverse package. Therefore, we can just load the tidyverse package instead of all three packages.\nThe code chunk below loads sf, tmap and tidyverse packages into R environment.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n2.3.2 Importing Geospatial Data into R\nWe can use st_read() of sf package to import MP14_SUBZONE_WEB_PL shapefile in R as a simple feature data frame called mpsz.\n\nmpsz <- st_read(dsn=\"data\\\\geospatial\",\n                layer='MP14_SUBZONE_WEB_PL')\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nExamine the content of mpsz using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nInterestingly, only the first ten records will be displayed.\nOn the other hand, we can also use head() to specify the number of rows to return (must be less than 10).\n\nhead(mpsz, 3)\n\nSimple feature collection with 3 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 28160.23 ymin: 28369.47 xmax: 32362.39 ymax: 30247.18\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO    SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N PLN_AREA_C\n1        1          1 MARINA SOUTH    MSSZ01      Y    MARINA SOUTH         MS\n2        2          1 PEARL'S HILL    OTSZ01      Y          OUTRAM         OT\n3        3          3    BOAT QUAY    SRSZ03      Y SINGAPORE RIVER         SR\n        REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84 29220.19\n2 CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06 29782.05\n3 CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96 29974.66\n  SHAPE_Leng SHAPE_Area                       geometry\n1   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n\n\n\n\n2.3.3 Import Attribute Data\nNext, I imported respopagsex2000to2018.csv file into RStudio and saved the file into an R dataframe called popagsex using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv('data\\\\aspatial\\\\respopagesextod2011to2020.csv')\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2.3.4 Data Preparation\nI am interested to visualise population demographics in the Year 2020. Before a thematic map can be prepared, I need to prepare a data table with Year 2020 values. The following variables will be required for this tasks: PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nPA: Planning Area\nSZ: Planning Subzone\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\nAs we can see, we will need to wrangle the data set and derive new columns like YOUNG and AGED.\n\n2.3.4.1 Data Wrangling\nThe following data wrangling and transformation functions were used:\n\npivot_wider() of tidyr package, and\n\nthis was used to pivot row values like age to columns. It “widens” data, increasing the number of columns and decreasing the number of rows.\n\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n2.3.4.2 Joining attribute data and geospatial data\nWe need to convert the PA and SZ values to uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nWrite the resulting file into a .rds file.\n\nwrite_rds(mpsz_pop2020, \"data\\\\rds\\\\mpszpop2020_amelia.rds\")"
  },
  {
    "objectID": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#choropleth-mapping-geospatial-data-using-tmap-package",
    "href": "Exercises/Hands-on_Ex1/Hands-on_Ex1.html#choropleth-mapping-geospatial-data-using-tmap-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R",
    "section": "2.4 Choropleth Mapping Geospatial Data Using tmap package",
    "text": "2.4 Choropleth Mapping Geospatial Data Using tmap package\nThere are two approaches to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n2.4.1 Plotting choropleth quickly using qtm()\nqtm() of tmap package provides a quick and concise visualisation.\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020,\n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n2.4.2 Creating choropleth map using tmap’s elements\nWhile qtm() can be used to get quick visualisation, the downside is that aesthetics of individual layers are harder to control. To get a high quality cartographic choropleth map, I will use tmap’s drawing elements.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nThe steps to creating the above map will be detailed in this sub-section.\n\n2.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons():\n\ntm_shape() defines the input data\ntm_polygons() draws the planning subzone polygons\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n2.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we assign the target variable such as Dependency to tm_polygons().\nBy default, missing values will be shaded in grey.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n2.4.2.3 Drawing a choropleth map using tm_fill() and tm_border()\n\ntm_fill(): shades the polygons using the default colour scheme\ntm_border(): Add borders to the polygons. The arguments are as follows:\n\nalpha specifies the transparency or opaqueness of the borders. By default, the alpha value of the col is used (normally 1 i.e. not transparent). There\ncol specifies the border colour,\nlwd specifies the border line width. The default is 1, and\nlty specifies the border line type. The default is “solid”.\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")+\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n2.4.3 Data classification methods of tmap\nChoropleth maps employ some methods of data classification so to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n2.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes (n = 5). This method classifies data into a certain number of categories with an equal number of units in each category.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used. This method sets the value ranges in each category equal in size.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nDIY: We can see that the distribution of quantile data classification method are more evenly distributed than equal data classification method.\nLet’s us examine other types of classification methods:\n\nTop Left: pretty (default),\nTop Right: equal,\nBottom Left: jenks and\nBottom Right: kmeans.\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_fill(c(\"DEPENDENCY\",\"DEPENDENCY\",\"DEPENDENCY\",\"DEPENDENCY\"),,\n              style = c(\"pretty\", \"equal\",\"jenks\",\"kmeans\"), \n              palette = list(\"Blues\",\"Oranges\",\"Reds\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))+\n  tm_borders(alpha = 0.2)\n\n\n\n\nWe can observe that pretty and equal gives similar distributions and not as even as the other two. Comparing jenks and kmeans classification methods, we can see that kmeans is more evenly distributed.\nDIY: The below code chunk uses the quantile classification method with different numbers of classes: 2 (top left), 6 (top right), 10 (bottom left), 20 (bottom right).\n\ntm_shape(mpsz_pop2020)+ \n  tm_fill(c(\"DEPENDENCY\",\"DEPENDENCY\",\"DEPENDENCY\",\"DEPENDENCY\"),\n              n = c(2,6,10,20),\n              style = c(\"quantile\", \"quantile\",\"quantile\",\"quantile\"), \n              palette = list(\"Blues\",\"Oranges\",\"Reds\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))+\n  tm_borders(alpha = 0.2)\n\n\n\n\nUnsurprisingly, we can see that as the number of classes increases, the more distributed the data is. Although the differences between the chart diminishes as the number of classes increase–for e.g., classes 10 and 20 are quite similar.\n\n\n2.4.3.2 \n2.4.3.2 Plotting choropleth map with custom break\nWe can set breakpoints using the breaks argument in tm_fill(). For tmap, breaks include a minimum and maximum. Therefore, to have n categories, n+1 elements must be specified in ascending order.\nFirst, some descriptive stats\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nWith reference to the above, we set break points at 0.00 (min), 0.60, 0.70, 0.80, 0.90, 1.00 (max).\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0.00, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n2.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package\n\n2.4.4.1 Using ColorBrewer Palette\nTo change the colour palette, we assign the preferred colour to palette argument of tm_fill() as shown below.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 6, \n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe can reverse the colour shades by adding ‘-’ in the palette argument.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n2.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohesive map. Map elements includes: objects to be mapped,title, scale bar, compass, margins and aspects ratios.\n\n2.4.5.1 Map Legend\nIn tmap, legend options allow us to change appearance, position and format of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n2.4.5.2 Map Style\nIn tmap, we can change a wide variety of layout settings using tmap_style(). The classic style is used here. Other available styles are: “white”, “gray”, “natural”, “cobalt”, “col_blind”, “albatross”, “beaver”, “bw”, “watercolor”\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\", \n          palette = \"-Greens\") + \n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n2.4.5.3 Cartographic Furniture\ntmap also provides arguments to draw other map furniture like compass, scale bar and grid lines.\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") + \n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2, \n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) + \n  tm_compass(type=\"8star\", size = 2) + \n  tm_scale_bar(width = 0.15) + \n  tm_grid(lwd = 0.1, alpha = 0.2) + \n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset to the default style, use the below code chunk.\n\ntmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n2.4.6 Drawing Small Multiple Choropleth Maps of Facet Maps\nSmall multiple maps or facet maps are composed of many maps arranged side-by-side or stacked vertically. Using facet maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n2.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nI created facet maps by defining col in tm_fill().\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(col = c(\"YOUNG\",\"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") + \n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) + \n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\nWe can also assign multiple values to aesthetic arguments like style and palette.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n              style = c(\"equal\", \"quantile\"), \n              palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n2.4.6.2 By defining a group-by variable in tm_facets()\nWe can create facet maps using tm_facets().\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by = \"REGION_N\",\n            free.coords = TRUE,\n            drop.units  = TRUE) + #instead of drop.shapes as it is deprecated\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"),\n            title.size = 20) + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nWe can create facet maps by creating multiple stand-alone maps and arranging them. In tmap_arrange(), arguments ncol specifies the number of columns to have and asp refers to the aspect ratio of each map.\n\n#Create stand-alone maps\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\n#Arrange Maps\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2) \n\n\n\n\nIf there are more than 2 maps, I can just add on for instance:\n\ntmap_arrange(youngmap, agedmap, youngmap, agedmap, asp=1.7, ncol=2)\n\n\n\n\n\n\n\n2.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend"
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, I learned how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, I was able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 The analytical question\nIn spatial policy, one of the main development objectives of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically.\nIf the answer is No. Then, our next question would be “is there sign of spatial clustering?”. And, if the answer for this question is Yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita or GDPPC) of Hunan Province, People Republic of China.\n\n\n2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n2.3 Setting the Analytical Tools\nThe code chunk below installs and loads sf, spdep, tmap and tidyverse packages into R environment. pacman() is a R package management tool. It provides intuitively named functions for the base functions.\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#importing-data-into-r-environment",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#importing-data-into-r-environment",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "3 Importing Data into R Environment",
    "text": "3 Importing Data into R Environment\nThe geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n3.1 Import shapefile into R\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features object of sf.\n\nhunan <- st_read(dsn = 'data\\\\geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nFrom the output, we can see that there are 88 multipolygons and 7 fields.\n\n\n3.2 Import csv file into R\n\nhunan2012 <- read_csv(\"data\\\\aspatial\\\\Hunan_2012.csv\", show_col_types = FALSE)\n\n\n\n3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan <- left_join(hunan, hunan2012)\n\nJoining, by = \"County\"\n\n\n\n\n3.4 Visualising Regional Development Indicator\nThe code chunk below is used to prepare two stand-alone choropleth maps to visualise the distribution of GDPPC 2012 by using gtm() of tmap package. The map on the left will be classified using equal intervals and the one on the right will be classified using quantiles.\nThen by using tmap_arrange() of tmap package, we will create a facet map.\nNote that:\n\nGDPPC refers to Gross Domestic Product per capita.\nqtm() allows us to plot thematic maps quickly.\n\n\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\", \n          n = 5, \n          style = 'equal') + \n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = 'Equal Interval Classification')\n\nquantile <- tm_shape(hunan) + \n  tm_fill(\"GDPPC\", \n          n = 5, \n          style = 'quantile') + \n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Equal Quantile Classification\")\n\ntmap_arrange(equal, quantile, asp = 1, ncol = 2)\n\n\n\n\nBased on the equal interval map, we can see that there is uneven distribution of GDPPC. However, the distribution is not that obvious when we use the quantile map. This example aims to alert us that depending on the method of representation e.g. equal or quantile, we may get different interpretations of the situation."
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#global-spatial-autocorrelation",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#global-spatial-autocorrelation",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "4 Global Spatial Autocorrelation",
    "text": "4 Global Spatial Autocorrelation\nIn this section, I learned how to compute global spatial autocorrelation statistics and how to perform spatial complete randomness test for global spatial autocorrelation.\n\n4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. By default, Queen contiguity is applied.\n\nwm_q <- poly2nb(hunan, \n                queen = TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two least connected area with only one neighbour.\n\n\n4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighbouring county then summing the weighted income values.\nWhile this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\nNote that other more robust options are available, notably style=“B”.\n\nrswm_q <- nb2listw(wm_q,\n                   style = \"W\",\n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n4.3 Global Spatial Autocorrelation: Moran’s I\nIn this section, I learned how to perform Moran’s I statistical testing by using moran.test() of spdep.\nmoran.test(x, listw, randomisation=TRUE, zero.policy=NULL,\n alternative=\"greater\", rank = FALSE, na.action=na.fail, spChk=NULL,\n adjust.n=TRUE, drop.EI2=FALSE)\nMoran’s I describe how features differ from the values in the study area as a whole. If the Moran I (Z-value is):\n\npositive (I>0): Clustered, observations tend to be similar\nnegative (I<0): Disperse, observations tend to be dissimilar\napproximately zero: observations arranged randomly over space\n\nWe will test the following hypothesis:\n\nH0: Observed spatial patterns of values is equally likely as any other spatial pattern i.e. data is randomly disbursed, no spatial pattern\nH1: Data is more spatially clustered than expected by chance alone.\n\n\nmoran.test(hunan$GDPPC,\n           listw = rswm_q,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nSince the p-value < 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone. Since Moran I statistic > 0.300, the observation are clustered, observations tend to be similar.\n\n4.3.1 Computing Monte Carlo Moran’s I\nIf we have doubts that the assumptions of Moran’s I are true (normality and randomisation), we can use a Monte Carlo simulation to perform a permutation test for Moran’s I.\nThe permutation tests consists of randomly reassigning the attribute values to a cell under the assumption of no spatial pattern. This random assignment is conducted n times. Each time, we will compute the Moran’s I to creating an empirical distribution of Moran’s I under H0.\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\nmoran.mc(x, listw, nsim, zero.policy=NULL, alternative=\"greater\",\n na.action=na.fail, spChk=NULL, return_boot=FALSE, adjust.n=TRUE)\nWe will test the following hypothesis using a one-tailed test:\n\nH0: Observed spatial patterns of values is equally likely as any other spatial pattern i.e. data is randomly disbursed, no spatial pattern\nH1: Data is more spatially clustered than expected by chance alone.\n\n\nset.seed(1234)\nbperm = moran.mc(hunan$GDPPC, \n         listw = rswm_q,\n         nsim = 999,\n         zero.policy = TRUE,\n         na.action = na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nSince the pseudo p-value < 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone.\n\n\n4.3.2 Visualising Monte Carlo Moran’s I\nWe can examine the simulated Moran’s I test statistics in greater detail through descriptive statistics and plotting the distribution of the statistical values as a histogram by using the code chunks below.\nThe mean gives the average of Moran’s I for all simulated distributions.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\nThe variance of Moran’s I for all simulated distributions can be computed using this code chunk.\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = \"Simulated Moran's I\")\nabline(v=0, \n       col = 'red')\nabline(v=0.30075, #actual moran's I\n       col = 'blue')\n\n\n\n\nFrom the output above, we can see that the distribution of the simulated values of Moran’s I is slightly skewed to the right. The blue line above indicates the actual value of Moran’s I, which is near the extremes of the distribution. This suggests that there is evidence of positive autocorrelation i.e. cluster. (Reference)\nWe can also plot the above graph using ggplot2 package. To do so, we must first convert the results into a dataframe.\n\ndf <- as.data.frame(bperm$res)\ncolnames(df) <- c(\"Simulated Moran's I\")\n\n\nggplot(df, aes(x=`Simulated Moran's I`)) + \n  geom_histogram(color = \"darkblue\", fill = \"lightblue\", bins = 20) +\n  ylab('Frequency')\n\n\n\n\n\n\n\n4.4 Global Spatial Autocorrelation: Geary’s C\nIn this section, I learned how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n4.4.1 Geary’s C Test\nGeary’s C describe how features differ from their immediate neighbours. If the Geary’s C (Z-value is):\n\nLarge (c>1): Dispersed, observations tend to be dissimilar\nSmall (c<1): Clustered, observations tend to be similar\nc = 1: observations arranged randomly over space\n\nWe will test the following hypothesis:\n\nH0: Observed spatial patterns of values is equally likely as any other spatial pattern i.e. data is randomly disbursed, no spatial pattern\nH1: Data is more spatially clustered than expected by chance alone\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\ngeary.test(x, listw, randomisation=TRUE, zero.policy=NULL,\n    alternative=\"greater\", spChk=NULL, adjust.n=TRUE)\n\ngeary.test(hunan$GDPPC, listw = rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nSince the p-value = 0.0001526 < 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone.\n\n\n4.4.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm = geary.mc(hunan$GDPPC,\n                 listw = rswm_q,\n                 nsim = 999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nSince the pseudo p-value = 0.001 < 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone.\n\n\n4.4.3 Visualising the Monte Carlo Geary’s C\nWe can examine the simulated Geary’s C test statistics in greater detail through descriptive statistics and plotting the distribution of the statistical values as a histogram by using the code chunks below.\nThe mean gives the average of Geary’s C for all simulated distributions.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\nThe variance of Geary’s C for all simulated distributions can be computed using this code chunk.\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = \"Simulated Geary's C\")\nabline(v=1, \n       col = 'red')\n\n\n\n\nFrom the output above, we can see that the distribution of the simulated values of Moran’s I fairly normally distributed."
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#spatial-correlogram",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#spatial-correlogram",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "5 Spatial Correlogram",
    "text": "5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\nIn spatial correlograms, the number of bins determines the distance range of each bin. The range is the maximum distance divided by the number of bins.\n\n5.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\nsp.correlogram(neighbours, var, order = 1, method = \"corr\",\n style = \"W\", randomisation = TRUE, zero.policy = NULL, spChk=NULL)\n# S3 method for spcor\nplot(x, main, ylab, ylim, ...)\n# S3 method for spcor\nprint(x, p.adj.method=\"none\", ...)\n\nMI_corr <- sp.correlogram(wm_q, #note that we used the original weights matrix\n                          hunan$GDPPC,\n                          order = 6,\n                          method = 'I',\n                          style = 'W')\nplot(MI_corr)\n\n\n\n\nNext, let’s examine the full analysis report and view which values are statistically significant.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the output, we see that with the exception of Lag 4, the rest of the results are statistically significant at the 95% level of confidence.\nThe correlogram depicts how the spatial autocorrelation changes with distance. From the chart, we can see that Moran’s I decreases when spatial lag increases. The first three lags ranges are statistically significant with a positive Moran’s I score while the last two (5 & 6) are statistically significant with a negative Moran’s I score. This tells us that there is quite a strong spatial autocorrelation for the first three distance classes. As distance increases beyond lag-4, there is negative autocorrelation.\n\n\n5.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr <- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6, \n                          method = \"C\",\n                          style = \"W\")\nplot(GC_corr)\n\n\n\n\nNext, let’s examine the full analysis report and view which values are statistically significant.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the output, we see that with the exception of Lag 3, 4 and 6, the rest of the results are statistically significant at the 95% level of confidence.\nThe correlogram depicts how the spatial autocorrelation changes with distance. From the chart, we can see that Geary’s C increases when spatial lag increases. This is unsurprising, given that Moran’s I and Geary’s C are inversely related.\nThe first two lags are statistically significant and below 1 while the last lag is statistically significant and above 1. This tells us that there is quite a strong spatial autocorrelation for the first two distance classes."
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#cluster-and-outlier-analysis",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#cluster-and-outlier-analysis",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "6 Cluster and Outlier Analysis",
    "text": "6 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city, local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, I learned how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’s I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n6.1 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviation of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nfips <- order(hunan$County) #order in alphabetical order, returns index of the variable\nprintCoefmat(data.frame(localMI[fips,],\n                        row.names=hunan$County[fips]), check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n6.1.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, I would append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task.\n\nhunan.localMI <- cbind(hunan, localMI) %>%\nrename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n6.1.2 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\nlocalMI.map <- tm_shape(hunan.localMI) + \n                  tm_fill(col = \"Ii\",\n                          style = 'pretty',\n                          title = \"Local Moran Statistics\") + \n                  tm_borders(alpha = 0.5)\nlocalMI.map\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n6.1.3 Mapping local Moran’s I p-values\nThe choropleth above shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\npvalue.map <- tm_shape(hunan.localMI) + \n                tm_fill(col = \"Pr.Ii\",\n                       breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                       palette = \"-Blues\",\n                       title = \"Local Moran's I p-values\") + \n                tm_borders(alpha = 0.5)\n\npvalue.map\n\n\n\n\nWe can see that only the darker blue areas are within the 0.05 significance level. This means that not all the local Moran’s I values are statistically significant at the 95% confidence level.\n\n\n6.1.4 Mapping both local Moran’s I values and\nWe can plot both the local Moran’s I values map and its corresponding p-values map next to each other using tmap_arrange() from tmap package.\n\ntmap_arrange(localMI.map, pvalue.map, asp = 1, ncol = 2)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nA positive Local Moran I Statistic corresponds to that area being part of a cluster–that means that its neighbours have similary high or low attribute values. A negative value for Local Moran I Statistic indicates that the area has neighboring features with dissimilar values, which means that this area is an outlier.\nWe should only look at counties where the p-value is significant at the 95% confidence level. And when we do, we can see that the Local Moran I Statistic of the darker green areas (on the left) are significant at the 95% confidence level. This means that it is part of a cluster. Its surrounding areas’ I statistic values are positive and statistically significant which indicates that they are also a cluster."
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#creating-a-lisa-cluster-map",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#creating-a-lisa-cluster-map",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "7 Creating a LISA Cluster Map",
    "text": "7 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations colour coded by type of spatial autocorrelation. Before we generate the LISA Cluster Map, we must plot the Moran scatterplot.\n\n7.1 Plotting Moran scatterplot\nThe Moran scatterplot illustrates the relationship between the values of the chosen attribute e.g., GDPPC at each location and the average value of the same attribute at neighbouring locations.\nWe can plot the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels = as.character(hunan$County),\n                  xlab = \"GDPPC 2012\",\n                  ylab = \"Spatially Lag GDPPC 2012\")\n\n\n\n\nThe Moran scatterplot is split into 4 quadrants:\n\nTop right corner (High-High): Autocorrelation Positive Cluster that belongs to counties with high GDPPC and are surrounded by other areas that have the higher than average level of GDPPC.\nTop left corner (Low-High): Autocorrelation Negative Cluster that belongs to counties with low GDPPC among high GDPPC neighboours\nBottom right corner (High-Low): Autocorrelation Negative Cluster that belongs to counties with high GDPPC among low GDPPC neighbours.\nBottom left corner (Low-Low): Autocorrelation Positive Cluster that belongs to counties with have low GDPPC among low GDPPC neighbours.\n\n\n\n7.2 Plotting Moran scatterplot with standardised variable\nWe can use scale() to center and scale the variable. Centering is done by subtracting the mean (omitting NAs) from the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\nThe above describes the Z-standardisation.\nThe below code chunk will create a new column Z.GDPPC in the hunan dataframe to store the standardised GDPPC values. The as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\n\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>%\n  as.vector\nhead(hunan,3)\n\nSimple feature collection with 3 features and 36 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 111.7027 ymin: 28.61762 xmax: 112.3013 ymax: 29.77344\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3 Shape_Leng Shape_Area  County    City\n1 Changde 21098 Anxiang      County   1.869074 0.10056190 Anxiang Changde\n2 Changde 21100 Hanshou      County   2.360691 0.19978745 Hanshou Changde\n3 Changde 21101  Jinshi County City   1.425620 0.05302413  Jinshi Changde\n  avg_wage deposite  FAI Gov_Rev Gov_Exp     GDP GDPPC     GIO   Loan  NIPCR\n1    31935   5517.2 3541  243.64  1779.5 12482.0 23667  5108.9 2806.9 7693.7\n2    32265   7979.0 8665  386.13  2062.4 15788.0 20981 13491.0 4550.0 8269.9\n3    28692   4581.7 4777  373.31  1148.4  8706.9 34592 10935.0 2242.0 8169.9\n   Bed    Emp  EmpR EmpRT Pri_Stu Sec_Stu Household Household_R NOIP Pop_R\n1 1931 336.39 270.5 205.9  19.584  17.819     148.1       135.4   53 346.0\n2 2560 456.78 388.8 246.7  42.097  33.029     240.2       208.7   95 553.2\n3  848 122.78  82.1  61.7   8.723   7.592      81.9        43.7   77  92.4\n    RSCG Pop_T    Agri Service Disp_Inc      RORP    ROREmp\n1 3957.9 528.3 4524.41   14100    16610 0.6549309 0.8041262\n2 4460.5 804.6 6545.35   17727    18925 0.6875466 0.8511756\n3 3683.0 251.8 2562.46    7525    19498 0.3669579 0.6686757\n                        geometry     Z.GDPPC\n1 POLYGON ((112.0625 29.75523... -0.04920595\n2 POLYGON ((112.2288 29.11684... -0.22834116\n3 POLYGON ((111.8927 29.6013,...  0.67940617\n\n\nIn the output above, we can see the new column added.\nWe can now plot the Moran scatterplot using the Z.GDPPC, the standardised GDPPC values.\n\nnci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels = as.character(hunan$County),\n                   xlab = \"Z-GDPPC 2012\",\n                   ylab = \"Spatially Lag z-GDPPC 2012\")\n\n\n\n\nIn the output above, we notice that the x-axis ranged has been scaled down as compared to the previous section.\n\n\n7.3 Preparing LISA map classes\nWe can prepare a LISA cluster map by following the steps:\n\n#create a vector \nquadrant <- vector(mode = \"numeric\", length = nrow(localMI))\n\nCenter attribute (GDPPC) around its mean.\n\nDV <- hunan$GDPPC - mean(hunan$GDPPC)\n\nIf DV > 0, it means that the GDPPC of the target county is higher than the mean.\nNext, centering the local Moran’s I around its mean.\n\nC_MI <- localMI[,1] - mean(localMI[,1])\n\nIf C_MI > 0, suggests clustering. If C_MI < 0 suggests an outlier or regular patterns.\nSet statistical significance level for the local Moran.\n\nsignif <- 0.05\n\nDefine the command lines for: high-high, low-low, low-high, high-low categories.\n\nquadrant[DV > 0 & localMI[,1] > 0] <- 4 #high-high \nquadrant[DV < 0 & localMI[,1] < 0] <- 2 #low-high\nquadrant[DV < 0 & localMI[,1] > 0] <- 1 #low-low\nquadrant[DV > 0 & localMI[,1] < 0] <- 3 #high-low\n\n#DV > 0 -> GDPPC of target higher than mean  \n#C_MI > 0 --> cluster | #C_MI < 0 --> outliers / regular pattern\n\nLastly, place non-significant Moran in the category 0.\n\nquadrant[localMI[,5]>signif] <- 0\n\nAlternatively, we can combine all the above steps into a single code chunk.\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nDV <- hunan$GDPPC - mean(hunan$GDPPC)     \nC_mI <- localMI[,1] - mean(localMI[,1])    \nsignif <- 0.05       \nquadrant[DV > 0 & localMI[,1] > 0] <- 4 #high-high\nquadrant[DV < 0 & localMI[,1] < 0] <- 2 #low-high\nquadrant[DV < 0 & localMI[,1] > 0] <- 1 #low-low\nquadrant[DV > 0 & localMI[,1] < 0] <- 3 #high-low\nquadrant[localMI[,5]>signif] <- 0\n\n\n\n7.4 Plotting LISA Map\nLet’s build the LISA map using the following code chunk.\n\n#Assign each county to its respective quardrant\nhunan.localMI$quadrant <- quadrant\n#Set the colours--one for each quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\") \n#Name the clusters 0 > 1 > 2 > 3 > 4\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(hunan.localMI) + \n  tm_fill(col = \"quadrant\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1],\n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) + \n  tm_borders(alpha = 0.5)\n\nLISAmap          \n\n\n\n\nWe can plot both the local Moran’s I map and LISA Map together.\n\ntmap_arrange(localMI.map, LISAmap, asp = 1, ncol = 2 )\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nWe can also visualise the GDPPC and LISAmap together.\n\ngdppc <- qtm(hunan, fill = \"GDPPC\")\ntmap_arrange(gdppc, LISAmap, asp = 1, ncol = 2)\n\n\n\n\nFrom the outputs above, counties shaded in light blue and light orange are outliers. The light blue regions are outliers, where they have lower GDPPC as compared to their neighbours, and vice versa for the region in orange. On the other hand, the regions that are dark blue and red are considered clusters.\nInterestingly, the dark blue area on the top right (LISA Chart) indicates that the area has lower GDPPC and is surrounded by areas with relatively low GDPPC as well. Examining the GDPPC chart on the left, we can see that indeed, the area has a relatively lower GDPPC, however, most of its neighbours seem to have relatively higher GDPPC."
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "8 Hot Spot and Cold Spot Area Analysis",
    "text": "8 Hot Spot and Cold Spot Area Analysis\nBeside detecting clusters and outliers, localised spatial statistics can also be used to detect hot spot and/or cold spot areas. Hot spot refers to areas that have higher values relative to its surroundings.\n\n8.1 Getis and Ord’s G-Statistics\nThe Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995) are alternative spatial statistics to detect spatial anomalies.\nIt looks at neighbours within a defined proximity to identify where either high or low values cluster spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n8.2 Deriving distance-based weight matrix\nGetis-Ord defines neighbours based on distance. This means we cannot use the previously derived Queen weight matrix and will need to derive a new set of weight matrix using distance as the measure.\nThere are two types of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n8.2.1 Deriving Polygon Centroids\nWe need to associate each polygon with a point before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: hunan. I need the coordinates in a separate data frame for this to work. To do this I will use a mapping function.\nThe mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of hunan. Our function will be st_centroid(). We will be using map_dbl() variation of map from the purrr package. purrr is loaded when we load tidyverse.\nTo get our longitude values we map the st_centroid() function over the geometry column of hunan and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, I used cbind to put longitude and latitude into the same object. We should check the first few observations to see if things are formatted correctly.\n\ncoords <- cbind(longitude, latitude)\nhead(coords, 5)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n\n\n\n\n8.2.2 Determine the cut-off distance\nWe need to determine the cut-off distance, within which, two areas are considered neighbours. To ensure that each region has at least one neighbour, we can set the upper limit of the distance band to the maximum first nearest neighbour distance. To do so, we can follow these steps:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nknearneigh(x, k=1, longlat = NULL, use_kd_tree=TRUE)\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe output tells us that the largest first nearest neighbour distance is 61.79km. Using this as the upper limit will ensure that all regions have at least one neighbour.\n\n\n8.2.3 Computing fixed distance-based weight matrix\nNow, we can compute the distance-based weight matrix with a distance boundary of 0 to 62km.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nFrom the output, we see that the average number of links is 3.68. This means that on average, the number of links each polygons has 3.68.\nNext, we will use nb2listw() to convert the nb object into a spatial weights object.\n\nwm62_lw <- nb2listw(wm_d62, style =\"B\")\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n\n8.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nTo control the numbers of neighbours directly, we can use k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\nAs a rule of thumb, we will set k = 8 so that the GDPPC is evaluated within the context of at least 8 neighbours.\n\nknn <- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nSince we set k=8, the average number of links is also 8 since each county has 8 neighbours.\nAgain, we will convert the nb object into a spatial weights object.\n\nknn_lw <- nb2listw(knn, style = \"B\")\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#computing-gi-statistics",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#computing-gi-statistics",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "9 Computing Gi Statistics",
    "text": "9 Computing Gi Statistics\n\n9.1 Gi Statistics using fixed distance\nTo compute the Gi statistics, we can use localG() from spdep package.\nThe Gi statistic value returned is a Z-value. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\n\ngi.fixed <- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nNext, we will join the Gi values to their corresponding hunan sf dataframe.\n\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n9.1.1 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix\n\ngdppc <- qtm(hunan, 'GDPPC')\n\nGimap <- tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\",\n          style = \"pretty\",\n          palette = '-RdBu', \n          title = 'local Gi') + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp = 1, ncol = 2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nAs mentioned earlier, greater Gi values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters. As we can see there is an area on the top left of the Gimap with high values of Gi. This area could be a hot-spots–areas of high values where other areas within a neighbourhood range also share high values.\n\n\n\n9.2 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knn_lw).\n\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n9.2.1 Mapping Gi values with adaptive distance weights\nTo visualise the locations of hot spot and cold spot areas, we can use the choropleth mapping functions of tmap package.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\",\n          style = \"pretty\",\n          palette = \"-RdBu\",\n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp = 1, ncol = 2)\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nSince the definition of neighbours changed, we can observe differences in identified cold/hot spots between the above chart and the one from the previous section."
  },
  {
    "objectID": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#references",
    "href": "Exercises/Hands-on_Ex2/Hands-on_Ex2.html#references",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "10 References",
    "text": "10 References\nStephanie Glen. “Moran’s I: Definition, Examples” From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/morans-i/\nTin Seong Kam. “4 Global and Local Measures of Spatial Autocorrelation” From R for Geospatial Data Science and Analytics https://r4gdsa.netlify.app/chap04.html"
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In this in-class exercise, I learned how to compute spatial weights using R. Detailed objectives are as follows::\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#study-area-and-data",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#study-area-and-data",
    "title": "In-class Exercise 1",
    "section": "2 Study Area and Data",
    "text": "2 Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012."
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#getting-started",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#getting-started",
    "title": "In-class Exercise 1",
    "section": "3 Getting Started",
    "text": "3 Getting Started\nThe code chunk below will install and load sf, spdep, tmap and tidyverse packages.\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#import-data-into-r-environment",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#import-data-into-r-environment",
    "title": "In-class Exercise 1",
    "section": "4 Import Data into R Environment",
    "text": "4 Import Data into R Environment\n\n4.1 Importing shape file into R\nThe code chunk below will import ESRI shape file into R.\n\nhunan <- st_read(dsn = \"data\\\\geospatial\",\n                layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nThe output shows that there are 88 multipolygon features and 7 fields.\n\n\n4.2 Import csv file into R\nThe code chunk below uses read_csv() of readr package to import Hunan_2012 csv file into R. The output R object is called hunan_2012 and is in R dataframe class.\n\nhunan2012<- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n4.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan <- left_join(hunan, hunan2012)\n\nJoining, by = \"County\"\n\n\n\n\n4.4 Visualising Regional Development Indicator\nThe code chunk below is used to prepare two stand-alone maps: a basemap and a choropleth map showing the distribution of GDPPC 2012 by using gtm() of tmap package. Then by using tmap_arrange() of tmap package, we will create a facet map.\nNote that:\n\nGDPPC refers to Gross Domestic Product per capita.\nqtm() allows us to plot thematic maps quickly.\n\n\nbasemap <- tm_shape(hunan) + \n  tm_polygons() +\n  tm_text(\"NAME_3\", size = 0.5)\n\ngdppc <- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap,gdppc, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#computing-contiguity-spatial-weight",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#computing-contiguity-spatial-weight",
    "title": "In-class Exercise 1",
    "section": "5 Computing Contiguity Spatial Weight",
    "text": "5 Computing Contiguity Spatial Weight\nIn this section, I learned how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. This function allows us to pass a “queen” argument that takes TRUE or FALSE as options. The default is set to TRUE, which means that the function will return a list of first order neighbours using the Queen criteria by default.\n\n5.1 Computing (Queen) contiguity based neighbours\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two least connected region with only one neighbour.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can use the code chunk below to retrieve county name of Polygon ID = 1. The output reveals that Polygon ID=1 is Anxiang county.\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nTo reveal the county names of Anxiang county’s five neighboring polygons, the code chunk will be used:\n\nhunan$County[c(2, 3,4, 57, 85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nAlternatively, we can combine the earlier steps to identify the five neighbours and get their names into one code chunk.\n\nhunan$County[wm_q[[1]]]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nNext, let’s retrieve the GDPPC of these five counties.\n\nnb1 <- wm_q[[1]]\ngdp1 <- hunan$GDPPC[nb1]\ngdp1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\n\n\n\n\n\n\nTip\n\n\n\nWe can combine them the names and GDPPC into a matrix by using cbind().\n\ncbind(hunan$County[wm_q[[1]]], gdp1)\n\n               gdp1   \n[1,] \"Hanshou\" \"20981\"\n[2,] \"Jinshi\"  \"34592\"\n[3,] \"Li\"      \"24473\"\n[4,] \"Nan\"     \"21311\"\n[5,] \"Taoyuan\" \"22879\"\n\n\n\n\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n5.2 Creating (ROOK) contiguity based on neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r <- poly2nb(hunan, queen = FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two least connected regions with only one neighbours.\n\n\n5.3 Visualising contiguity weights\nA connectivity graph takes a point and displays a line to each neighboring point. Since we are working with polygons, we will need to get points in order to make our connectivity graphs. The most commonly used method to get points is to take the centroids of the polygons. We will calculate these in the sf package before moving onto the graphs.\n\n5.3.1 Getting Latitude and Longitude of Polygon Centroids\nWe need to associate each polygon with a point before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: hunan. I need the coordinates in a separate data frame for this to work. To do this I will use a mapping function.\nThe mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of hunan. Our function will be st_centroid(). We will be using map_dbl() variation of map from the purrr package. purrr is loaded when we load tidyverse.\nTo get our longitude values we map the st_centroid() function over the geometry column of hunan and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, I used cbind to put longitude and latitude into the same object. We should check the first few observations to see if things are formatted correctly.\n\ncoords <- cbind(longitude, latitude)\nhead(coords, 5)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n\n\n\n\n5.3.2 Plotting Queen contiguity based neighbours map\nWith the coordinates for each county, we can plot the neighbors through a connectivity graph.\nNotes on the arguments used in plot():\n\npch: plotting symbols available. ‘19’ refers to solid circles. Detailed list can be found here.\nadd: adds the current plot onto the previous plot\ncex: a numerical value giving the amount by which plotting text and symbols should be magnified relative to the default\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n5.3.3 Plotting Rook contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n5.3.4 Plotting both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\", main=\"Queen Contiguity\")\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main=\"Rook Contiguity\")\n\n\n\n\nSince the queen criterion is somewhat more encompassing than rook criterion, the number of neighbors according to the queen criterion will always be at least as large as for the rook criterion. We can observe this in the above maps as well.\n\nRook criterion: defines neighbors by the existence of a common edge between two spatial units.\nQueen criterion: defines neighbors as spatial units sharing a common edge or a common vertex."
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#computing-distance-based-neighbours",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#computing-distance-based-neighbours",
    "title": "In-class Exercise 1",
    "section": "6 Computing distance based neighbours",
    "text": "6 Computing distance based neighbours\nIn this section, I learned how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\ndnearneigh(x, d1, d2, row.names = NULL, longlat = NULL, bounds=c(\"GE\", \"LE\"),\n use_kd_tree=TRUE, symtest=FALSE, use_s2=packageVersion(\"s2\") > \"1.0.7\", k=200,\n dwithin=TRUE)\n\n6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nknearneigh(x, k=1, longlat = NULL, use_kd_tree=TRUE)\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords, k=1)) #default k = 1\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nhead(k1dists,5)\n\n[1] 25.53398 43.03114 25.53398 29.28480 29.28480\n\n\n\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n6.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh()as shown below.\n\nwm_d62 <- dnearneigh(coords,0,62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nFrom the output, we see that the average number of links is 3.68. This means that on average, the number of links each polygons has 3.68.\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nn.comp.nb() finds the number of disjoint connected subgraphs in the graph depicted by nb.obj - a spatial neighbours list object.\n\nn_comp <- n.comp.nb(wm_d62)\nn_comp\n\n$nc\n[1] 1\n\n$comp.id\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[77] 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n6.2.1 Plotting fixed distance weight matrix\nNext, I will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border = 'lightgrey') #base map\nplot(wm_d62, coords, add = TRUE) #neighbours within 62km\nplot(k1, coords, add = TRUE, col = 'red', length = 0.8) #1st nearest neigbours\n\n\n\n\n\npar(mfrow = c(1,2))\nplot(hunan$geometry, border = \"lightgrey\",main=\"1st nearest neighbours\" )\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.88, )\n\nplot(hunan$geometry, border = \"lightgrey\", main = \"Distance Link\")\nplot(wm_d62, coords, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n6.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 <- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSince we set k=6, the average number of links is also 6 since each county has 6 neighbours.\nSimilarly, we can display the content of the matrix by using str(). Notice that each county has exactly 6 neighbours.\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\n6.3.1 Plotting distance based neighbours\nWe can plot the weight matrix using the code chunk below.\n\nplot(hunan$geometry, color = 'lightgrey')\n\nWarning in title(...): \"color\" is not a graphical parameter\n\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = 'red')"
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#weights-based-on-idw",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#weights-based-on-idw",
    "title": "In-class Exercise 1",
    "section": "7 Weights based on IDW",
    "text": "7 Weights based on IDW\nIn this section, I learned how to derive a spatial weight matrix based on Inversed Distance method.\nFirstly, let’s compute the distances between areas by using nbdists() of spdep.\n\ndist <- nbdists(wm_q, coords, longlat = TRUE)\nids <- lapply(dist, function(x) 1/(x)) #i.e. inverse distance = 1/distance\nhead(ids,10)\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n\n\n7.1 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\nWhile this is the most intuitive way to summarise the neighbors’ values, it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\nFor this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\n\nrswm_q <- nb2listw(wm_q, style = \"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nTo see the weight of the first polygon’s four neighbors type:\n\nrswm_q$weights[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nEach neighbor is assigned a 0.2 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids <- nb2listw(wm_q, glist = ids, style = \"B\", zero.policy = TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#application-of-spatial-weight-matrix",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#application-of-spatial-weight-matrix",
    "title": "In-class Exercise 1",
    "section": "8 Application of Spatial Weight Matrix",
    "text": "8 Application of Spatial Weight Matrix\nIn this section, I learned how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average and\nspatial window sum.\n\n\n8.1 Spatial lag with row-standardised weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecall that in the previous section, we retrieved the GDPPC of these five counties by using the code chunk below\n\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: Can you see the meaning of Spatial lag with row-standardized weights now?\n\nmean(nb1)\n\n[1] 24847.2\n\n\nTaking the mean of the GDPPC of the five neighbours of county 1 will return the same value as the first value of GDPPC.lag.\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list <- list(hunan$County, lag.listw(rswm_q, hunan$GDPPC))\nlag.res <- as.data.frame(lag.list)\ncolnames(lag.res) <- c(\"County\", \"lag GDPPC\")\nhead(lag.res)\n\n   County lag GDPPC\n1 Anxiang  24847.20\n2 Hanshou  22724.80\n3  Jinshi  24143.25\n4      Li  27737.50\n5   Linli  27270.25\n6  Shimen  21248.80\n\n\n\nhunan <- left_join(hunan, lag.res)\n\nJoining, by = \"County\"\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 36 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3 Shape_Leng Shape_Area  County    City\n1 Changde 21098 Anxiang      County   1.869074 0.10056190 Anxiang Changde\n2 Changde 21100 Hanshou      County   2.360691 0.19978745 Hanshou Changde\n3 Changde 21101  Jinshi County City   1.425620 0.05302413  Jinshi Changde\n4 Changde 21102      Li      County   3.474325 0.18908121      Li Changde\n5 Changde 21103   Linli      County   2.289506 0.11450357   Linli Changde\n6 Changde 21104  Shimen      County   4.171918 0.37194707  Shimen Changde\n  avg_wage deposite     FAI Gov_Rev Gov_Exp     GDP GDPPC     GIO   Loan  NIPCR\n1    31935   5517.2  3541.0  243.64  1779.5 12482.0 23667  5108.9 2806.9 7693.7\n2    32265   7979.0  8665.0  386.13  2062.4 15788.0 20981 13491.0 4550.0 8269.9\n3    28692   4581.7  4777.0  373.31  1148.4  8706.9 34592 10935.0 2242.0 8169.9\n4    32541  13487.0 16066.0  709.61  2459.5 20322.0 24473 18402.0 6748.0 8377.0\n5    32667    564.1  7781.2  336.86  1538.7 10355.0 25554  8214.0  358.0 8143.1\n6    33261   8334.4 10531.0  548.33  2178.8 16293.0 27137 17795.0 6026.5 6156.0\n   Bed    Emp  EmpR EmpRT Pri_Stu Sec_Stu Household Household_R NOIP Pop_R\n1 1931 336.39 270.5 205.9  19.584  17.819     148.1       135.4   53 346.0\n2 2560 456.78 388.8 246.7  42.097  33.029     240.2       208.7   95 553.2\n3  848 122.78  82.1  61.7   8.723   7.592      81.9        43.7   77  92.4\n4 2038 513.44 426.8 227.1  38.975  33.938     268.5       256.0   96 539.7\n5 1440 307.36 272.2 100.8  23.286  18.943     129.1       157.2   99 246.6\n6 2502 392.05 329.6 193.8  29.245  26.104     190.6       184.7  122 399.2\n    RSCG Pop_T    Agri Service Disp_Inc      RORP    ROREmp lag GDPPC\n1 3957.9 528.3 4524.41   14100    16610 0.6549309 0.8041262  24847.20\n2 4460.5 804.6 6545.35   17727    18925 0.6875466 0.8511756  22724.80\n3 3683.0 251.8 2562.46    7525    19498 0.3669579 0.6686757  24143.25\n4 7110.2 832.5 7562.34   53160    18985 0.6482883 0.8312558  27737.50\n5 3604.9 409.3 3583.91    7031    18604 0.6024921 0.8856065  27270.25\n6 6490.7 600.5 5266.51    6981    19275 0.6647794 0.8407091  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_gdppc <- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp = 1, ncol =2)\n\n\n\n\n\n\n8.2 Spatial lag as a sum of neighbouring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw() to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply(), which applies a function across each value in the neighbors structure.\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\nhead(b_weights)\n\n[[1]]\n[1] 1 1 1 1 1\n\n[[2]]\n[1] 1 1 1 1 1\n\n[[3]]\n[1] 1 1 1 1\n\n[[4]]\n[1] 1 1 1 1\n\n[[5]]\n[1] 1 1 1 1\n\n[[6]]\n[1] 1 1 1 1 1\n\nb_weights2 <- nb2listw(wm_q,\n                       glist = b_weights,\n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw() to compute a lag variable from our weight and GDPPC.\n\nlag_sum <- list(hunan$County, lag.listw(b_weights2, hunan$GDPPC))\nlag.res <- as.data.frame(lag_sum)\ncolnames(lag.res) <- c(\"County\", \"lag_sum GDPPC\")\nhead(lag.res)\n\n   County lag_sum GDPPC\n1 Anxiang        124236\n2 Hanshou        113624\n3  Jinshi         96573\n4      Li        110950\n5   Linli        109081\n6  Shimen        106244\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: Can you understand the meaning of Spatial lag as a sum of neighbouring values now?\nLike the term suggests, the spatial lag is now the sum of neighbouring values. We can verify by taking the sum of county 1’s neighbours’ GDPPC.\n\nsum(nb1)\n\n[1] 124236\n\n\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunan <- left_join(hunan, lag.res)\n\nJoining, by = \"County\"\n\n\nNow, we can plot both GDPPC and Spatial Lag Sum GDPPC for comparison.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc <- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\n\n8.3 Spatial Window Average\nThe spatial window average uses row-standardises weights and includes the diagonal element. To do this in R, we need to go back to the neighbours structure and add the diagonal element before assignment weights. To begin, we assign wm_q to a new variable because we will directly alter its structure to add the diagonal elements.\n\nwm_q1 <- wm_q\n\nTo add the diagonal element to the neighbour list, we can use include.self() from spdep.\n\ninclude.self(wm_q1)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNow obtain weights with nb2listw().\n\nwm_q1 <- nb2listw(wm_q1)\nwm_q1\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nWe can create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gdppc <- lag.listw(wm_q1,\n                             hunan$GDPPC)\n\nlag.list.wm_q1 <- list(hunan$County, lag_w_avg_gdppc)\nlag_wm_q1.res <- as.data.frame(lag.list.wm_q1)\ncolnames(lag_wm_q1.res) <- c(\"County\", \"lag_window_avg GDPPC\")\n\nLet’s append the lag_window_avg GDPPC values onto hunan sf data.frame.\n\nhunan <- left_join(hunan, lag_wm_q1.res)\n\nJoining, by = \"County\"\n\n\nFinally, let’s compare the GDPPC and lag_window_avg GDPPC.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nw_avg_gdppc <- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(gdppc, w_avg_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\n\n8.4 Spatial Window Sum\nThe spatial window sum is the country part of window average, but without using row-standardised weights. To do this, we assign binary weights to the neighbour structure that includes the diagonal element.\n\nwm_q1 <- wm_q\n\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\ninclude.self(wm_q1)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\n\nwm_q1\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights <- lapply(wm_q1, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 <- nb2listw(wm_q1,\n                       glist = b_weights,\n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc <- list(hunan$County, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc.res <- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) <- c(\"County\", \"w_sum GDPPC\")\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan <- left_join(hunan, w_sum_gdppc.res)\n\nJoining, by = \"County\"\n\n\nLastly, qtm() of tmap package is used to plot the GDPPC and lag_sum GDPPC map next to each other for quick comparison.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nw_sum_gdppc <- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(gdppc, w_sum_gdppc, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Exercises/In-class_Ex1/In-class_Ex1.html#references",
    "href": "Exercises/In-class_Ex1/In-class_Ex1.html#references",
    "title": "In-class Exercise 1",
    "section": "9 References",
    "text": "9 References\n\nhttps://r4gdsa.netlify.app/chap03.html#spatial-lag-as-a-sum-of-neighboring-values\nhttps://spatialanalysis.github.io/handsonspatialdata/contiguity-based-spatial-weights.html"
  },
  {
    "objectID": "hands-on.html",
    "href": "hands-on.html",
    "title": "Hands-on Exercises",
    "section": "",
    "text": "This is the section page where you may easily navigate to all the hands-on exercises that I have done."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "In this webpage, I am going to share with you my learning journey of geospatial analytics."
  },
  {
    "objectID": "Exercises/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Exercises/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good",
    "section": "",
    "text": "Water is an essential resource that not only supports life but also drives economic development. According to the World Bank, approximately 2 billion people in the world do not have safely managed drinking water services and 3.6 billion people lack safely managed sanitation services. Developing countries are most affected by the shortage of water. The lack of ground water threatens their fight against poverty, food and water security and socio-economic development.\n\n\nOrganisations like the World Bank, UNICEF and Water Point Data Exchange (WPdx) have various plans and schemes in place to combat this issue. In particular, WPdx has it in their mission to unlock potential of water point data to improve rural water services through evidence-based decision-making. It maintains a global data repository for data collected from rural areas at the water point or small water scheme level. A notable point is that data is formatted according to the WPdx Data Standard before being uploaded and published onto the repository. Using these information, decision support tools linked to the repository will be able to perform advanced analyses seamlessly.\nReferences:\nhttps://www.waterpointdata.org/about/\nhttps://www.worldbank.org/en/topic/water/overview\nhttps://www.unwater.org/publications/un-world-water-development-report-2022\n\n\n\nGeospatial analytics hold tremendous potential to address complex societal problems like water shortage. In this study, I will apply appropriate global and local measures of spatial association techniques to reveal the spatial patterns of non-functional water points.\n\n\n\nThe focus of this study would be Nigeria. Nigeria is located in West Africa and is the most populous country in Africa. UNICEF estimates that one third of Nigeria children do not have sufficient water to meet daily needs.\nhttps://www.unicef.org/nigeria/press-releases/nearly-one-third-nigerian-children-do-not-have-enough-water-meet-their-daily-needs"
  },
  {
    "objectID": "Exercises/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "href": "Exercises/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 Setting the Analytical Tools\nThe code chunk below installs and loads sf, spdep, tmap and tidyverse packages into R environment. pacman() is a R package management tool. It provides intuitively named functions for the base functions.\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Exercises/Take-home_Ex1/Take-home_Ex1.html#data-preparation",
    "href": "Exercises/Take-home_Ex1/Take-home_Ex1.html#data-preparation",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good",
    "section": "3 Data Preparation",
    "text": "3 Data Preparation\n\n3.1 Data\nAs mentioned in the earlier section, the focus of this study is Nigeria. Two data sets will be used in this study. They are:\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area or LGA) polygon feature GIS data. The data was obtained from geoBoundaries.\nWPdx+ data set that was obtained from Water Point Data Exchange (WPdx). It consists of water point related data from rural areas at the water point or small water scheme level. The entire set of data includes countries other than Nigeria. Hence, we will be performing data pre-processing to extract the relevant data.\n\n\n\n\n\n\nTip\n\n\n\nThe raw WPdx+ data file is 427mb and exceeds the upload limit of Github. In the next section, we will extract the relevant and necessary information, extract it into a .rds file and use the file for subsequent analysis. The raw file will not be pushed to Github to avoid crashing the Github repository.\n\n\n\n\n\n3.2 Importing the data into R Environment\nThe geospatial data is in ESRI shapefile format and the attribute table is in csv format.\n\n3.2.1 Importing Geospatial data into R\nThe code chunk below uses st_read() function of sf package to import geoBoundaries-NGA-ADM2 shapefile into R as a polygon feature data frame. The imported shapefile will be a simple features object of sf.\n\nnigeria <- st_read(dsn = \"data\\\\geospatial\",\n                   layer = \"geoBoundaries-NGA-ADM2\")\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nFrom the output, we can see that there are 774 multipolygons features with 5 fields. nigeria is in WGS 84 coordinates system. The bounding box provides the x extend and y extend of the data.\nTo learn more about the attribute information, we can apply glimpse() of dplyr package.\n\nglimpse(nigeria)\n\nRows: 774\nColumns: 6\n$ shapeName  <chr> \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\", \"Abaka…\n$ Level      <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ shapeID    <chr> \"NGA-ADM2-72505758B79815894\", \"NGA-ADM2-72505758B67905963\",…\n$ shapeGroup <chr> \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NG…\n$ shapeType  <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((7.401109 5...., MULTIPOLYGON (…\n\n\nThe printout above details the data type of each field. For instance, $ shapeName is in character data type.\n\n\n3.2.2 Importing attribute data into R\nThe WPdx+ dataset has 70 columns and 406,566 rows.\n\nwpdx <- read_csv(\"data\\\\aspatial\\\\Water_Point_Data_Exchange.csv\", show_col_types = FALSE)\n\nFollowing the warning prompt, we can examine the imported data using the code chunk below.\n\nproblems(wpdx)\n\nThe output reveals that there is a mismatched in the expected imported values and the actual imported values. It seems that R expected boolean data types but got character data types instead. Let’s find out which columns 19 and 20 are, and the data type by using glimpse() of dplyr package to view the attribute information.\n\nglimpse(wpdx)\n\nglimpse() allows us to see all columns in the data frame which is very helpful since we have 70 columns. Columns 19 and 20 correspond to #rehab_year and #rehabilitator. We can see that they are in the lgl data type, which means that they expect logical values. We now understand that R prompted the message to warn us that the data is in the incorrect data type. We will not need to act on this warning for now as we have no use for these columns and will remove them later.\n\n\n\n3.3 Data Wrangling\nThe entire dataset is large, and we need only to extract the relevant information required for the analysis. The focus of the study is Nigeria and the analysis will be done at the Level-2 Administrative Boundary (or LGA) level. Therefore, we will be performing steps to:\n\nExtract data belonging to Nigeria and\nGroup water points according to their functional status at the LGA level.\n\nBesides the above, we will also be performing data preparation and wrangling techniques to surface data issues and resolve them prior to the analysis. Before we start our data wrangling, it would be useful to inspect the metadata to understand what each column represents.\n\n3.3.1 Extract data belonging to Nigeria\nTo learn which column(s) to use to filter for Nigeria’s data, we can inspect the metadata. We will not display the entire metadata here as it is lengthy. However, here is an excerpt of some columns:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\n#clean_country_name\nCleaned version of the country name based on provided GPS coordinates.\n\n\n#clean_adm1\nCleaned version of the Primary Administrative Division data based on provided GPS coordinates and GADM boundaries.\n\n\n#clean_adm2\nCleaned version of the Secondary Administrative Division data based on provided GPS coordinates and GADM boundaries.\n\n\n#status_id\nIdentify if any water is available on the day of the visit, recognizing that it may be a limited flow.\n\n\n#status_clean\nCategorized version of the #status parameter. Based on terms from the #status entry, status_clean includes 5 categories: Fully functional, Functional but needs repair, Non functional and needs repair, Non functional due to dry season, Abandoned and Other. These categories will continue to evolve and will be refined in future updates.\n\n\n#status\nStatus of the physical/mechanical condition of the water point.\n\n\n\nBased on the above, we can use #clean_country_name to filter out rows belonging to Nigeria. This can be done using the code chunk below. The filtered data set will be saved as wpdx_nigeria. We can also inspect the first few rows of the data by using head().\n\nwpdx_nigeria <- wpdx %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\nhead(wpdx_nigeria)\n\nLet’s use dim() to reveal the dimensions of the wpdx_nigeria.\n\ndim(wpdx_nigeria)\n\nThe print out reveals that wpdx_nigeria has 95,008 rows and 70 columns.\n\n\n3.3.2 Resolving Misspellings\n#clean_adm2 gives us the location of the water point at the LGA level and #status_clean provides us the status of the water point. Therefore, we will use these two variables to group our data.\nBefore we do that, let’s inspect the variables. Using the code chunk below, we use count() dplyr package to count the frequency of the each location and/or category.\n\ncount(wpdx_nigeria, `#clean_adm2`)\n\nFrom the above output for #clean_adm2, we can see that there are 753 LGA. There seems to be no duplicates or misspellings which is good.\n\ncount(wpdx_nigeria, `#status_clean`)\n\nFrom the output above, we observe three issues for the #status_clean column: misspellings, missing data and incorrect number of categories. Let’s tackle misspellings first.\nWe can easily see that there two similar categories “Non functional due to dry season” = “Non-Functional due to dry season”. One is spelled with a dash and one without. Let’s correct this by using the recode() from dplyr package.\n\n#recode \nwpdx_nigeria_clean <- wpdx_nigeria %>%\n  mutate(`#status_clean` = recode(`#status_clean`, \"Non functional due to dry season\" = \"Non-Functional due to dry season\"))\n\n#re-run the frequency count\ncount(wpdx_nigeria_clean, `#status_clean`)\n\nFrom the above, we can confirm that the categories has been recoded.\nIncorrect number of categories\nRecall that the metadata specifies that there would be 5 categories in the #status_clean: (1) Fully functional, (2) Functional but needs repair, (3) Non functional and needs repair, (4) Non functional due to dry season, (5) Abandoned and Other. In reality, we can see that there are 7 categories.\nLet’s aggregate some categories to align. We will merge:\n\n‘Functional but not in use’ with ‘Functional’ under ‘Functional’\n‘Abandoned/Decommissioned’ and ‘Abandoned’ under ‘Abandoned and Others’\n\n\n#recode\nwpdx_nigeria_clean <- wpdx_nigeria_clean %>%\n  mutate(`#status_clean` = recode(`#status_clean`,\n                                  \"Functional but not in use\" = \"Functional\",\n                                  \"Abandoned/Decommissioned\" = \"Abandoned and Others\", \n                                  \"Abandoned\" = \"Abandoned and Others\"\n                                  ))\n#re-run the frequency count\ncount(wpdx_nigeria_clean, `#status_clean`)\n\nFrom the above, we can confirm that the values have been recoded correctly. We now have 5 categories in the #status_clean column: (1) Functional, (2) Functional but needs repair, (3) Non functional, (4) Non functional due to dry season, (5) Abandoned and Others. This is more or less aligned with the metadata description.\n\n\n3.3.3 Missing values\nWe also note that there are 10,656 missing values in the #status_clean column. While the amount of missing values is more than 5% of the rows, we will drop them as they are not useful in helping us in our analysis of the functional and non-functional water points.\n\n#recode\nwpdx_nigeria_clean <- wpdx_nigeria_clean %>%\n  drop_na(`#status_clean`)\n\n#re-run the frequency count\ncount(wpdx_nigeria_clean, `#status_clean`)\n\n\n\n3.3.4 Derive new feature for analysis\nIn our study, we would like know functional and non-functional water points. So we will create a new column that states whether the water point if functional or not. For our analysis, we will recode some values e.g., ‘Functional but needs repair’ to be under ‘Functional’ and ‘Abandoned and Others’ to ‘Non-Functional’ as well.\n\n#recode\nwpdx_nigeria_clean <- wpdx_nigeria_clean %>%\n  mutate(`Functional_Status` = `#status_clean`) %>%\n  mutate(`Functional_Status` = recode(`Functional_Status`,\n         \"Functional but needs repair\" = \"Functional\",\n         \"Non-Functional due to dry season\" = \"Non-Functional\",\n         \"Abandoned and Others\" = \"Non-Functional\"))\n  \n#re-run the frequency count\ncount(wpdx_nigeria_clean, `Functional_Status`)\n\n\n\n3.3.5 Rename Columns\nLet us rename #clean_adm2 to LGA.\n\nwpdx_nigeria_clean <- wpdx_nigeria_clean %>% \n    rename(`LGA` = `#clean_adm2`)\n\n\n\n3.3.6 Drop Unwanted Columns\n\nwpdx_nigeria_simple <- subset(wpdx_nigeria_clean , \n                              select = c(\"LGA\", \"#lat_deg\", \n           \"#lon_deg\",\"#water_source_clean\",\n           \"#water_source_category\", \"#distance_to_primary_road\",\n            \"#distance_to_secondary_road\", \"#distance_to_tertiary_road\",\n            \"#distance_to_city\", \"#distance_to_town\", \"usage_capacity\",\n            \"is_urban\", \"cluster_size\", \"Functional_Status\")\n)\n\n\n\n3.3.7 Creating a simple feature data frame\nNext, we will create a simple feature data frame from wpdx_nigeria_simple. This is done using the code chunk below.\n\nwpdx_nigeria_sf <- st_as_sf(wpdx_nigeria_simple, \n                    coords = c(\"#lon_deg\",\"#lat_deg\"), \n                    crs=4326) %>%\n  st_transform(crs = 26391)\n\nWe can examine the content of this newly created simple feature data frame using the following code chunk.\n\nglimpse(wpdx_nigeria_sf)\n\nNotice that a new column called geometry has been added and the original #lon_deg and #lat_deg columns have been removed.\n\n\n3.3.8 Transforming the CRS\nWe will transform nigeria from geographic coordinate system to projected coordinate system. We need to do this transformation because the geographic coordinate system is inappropriate if the analysis require the use of distance and/or area measurements.\nFor the nigeria simple feature data frame, the output of the code chunk below tells us that it is in the wgs84 coordinate system.\n\nst_geometry(nigeria)\n\nThere are three Projected Coordinate Systems of Nigeria: EPSG: 26391, 26392, and 26303. For this study, we will be EPSG 26391. We can use the st_transform() of the sf package to reproject nigeria from one coordinate system to another coordinate system mathematically.\n\nnigeria26391 <- st_transform(nigeria, \n                             crs = 26391)\n\nNext, let us display the content of nigeria26391 sf data frame as shown below.\n\nst_geometry(nigeria26391)\n\nNotice that it is in projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems.\nLet us also rename shapeName to LGA.\n\nnigeria26391 <- nigeria26391 %>% \n    rename(`LGA` = `shapeName`)\n\n\n\n\n3.4 Export to .rds\nNext, let’s save our clean files into .rds files. This would enable us to shorten the loading time and we can avoid uploading the large raw files onto GitHub.\nThe code chunk below creates two new .rds file, one for the spatial and attribute file.\n\nwrite_rds(wpdx_nigeria_sf, \"data\\\\rds\\\\wpdx_nigeria_sf_clean.rds\")\nwrite_rds(nigeria26391, \"data\\\\rds\\\\nigeria_26391_clean.rds\")\n\n\n\n\n\n\n3.5 Point in Polygon Count\n\nfunctional <- wpdx_nigeria_sf %>%\n  filter(`Functional_Status` == 'Functional')\n\nnon_functional <- wpdx_nigeria_sf %>%\n  filter(`Functional_Status` == 'Non-Functional')\n\nNext, we can count the number of water points in each LGA using the following code chunk. Two operations are happening at the same time. First, the code chunk identifies water points located inside each LGA by using st_intersects(). Next, length() of Base R is used to calculate the number of water points that fall inside each LGA.\n\nnigeria26391$Functional_WP <- lengths(st_intersects(nigeria26391, functional))\nnigeria26391$Non_Functional_WP <- lengths(st_intersects(nigeria26391, non_functional))"
  },
  {
    "objectID": "Exercises/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "href": "Exercises/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good",
    "section": "4 Exploratory Data Analysis",
    "text": "4 Exploratory Data Analysis\n\n4.1 Distribution of Water Point Density in LGA using Histograms\nWe are interested to find out the distributions of water points in the LGAs in Nigeria. To do this, we must first compute the area of LGA and then the water point density.\nThe code chunk below uses st_area() of sf package to derive the area of each subzone. We are creating a new column Area to store the area values.\n\nnigeria26391$Area <- nigeria26391 %>%\n  st_area()\n\n\nnigeria26391 <- nigeria26391 %>%\n  mutate(`Func_Water_Point_Density` = (`Functional_WP` / Area * 1000000))\n\n\nggplot(data=nigeria26391,\n       aes(x=as.numeric(`Func_Water_Point_Density`))) +\n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill = \"pink\") + \n  labs(title = \"Are functional water points evenly distributed in Nigeria?\",\n       subtitle = \"While there are many LGAs less than one waterpoint per km sq, there is 1 LGA with 10 \\nfunctional water points per km sq.\",\n       x = \"Functional Water Point density (per km sq)\",\n       y = \"Frequency\")\n\n\n\n\nWe can repeat the same for non-functional water points.\n\nnigeria26391 <- nigeria26391 %>%\n  mutate(`NonFunc_Water_Point_Density` = (`Non_Functional_WP` / Area * 1000000))\n\nggplot(data=nigeria26391,\n       aes(x=as.numeric(`NonFunc_Water_Point_Density`))) +\n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill = \"orange\") + \n  labs(title = \"Are non-functional water points evenly distributed in Nigeria?\",\n       subtitle = \"It would be good to know that most areas have less than one non-functional water point \\nper km sq.\",\n       x = \"Non-Functional Water Point density (per km sq)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n4.2 Water Point Count vs LGA Density\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nf <- tm_shape(nigeria26391) + \n  tm_fill(\"Functional_WP\",\n          n = 5,\n          style = 'quantile') +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = 'Quantile Classification of \\nFunctional Water Points in Nigeria',\n            main.title.size = 1,\n            main.title.position = 'center',\n            legend.height = 0.45) + \n  tm_compass(position = c('left','bottom'))\n\nnf <- tm_shape(nigeria26391) + \n  tm_fill (\"Non_Functional_WP\",\n             n = 5, \n             style = 'quantile') + \n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = 'Quantile Classification of \\nNon-Functional Water Points in Nigeria', \n            main.title.size = 1, \n            main.title.position = 'center',\n            legend.height = 0.45) + \n  tm_compass(position = c('left','bottom'))\n\n\ntmap_arrange(f, nf, asp = 1, ncol = 2)\n\n\n\n\nBased on what we can see above, the north eastern region and southern region of Nigeria have relatively low number of functional and non-functional water points. The top center part of the Nigeria seems to have a large number of functional water points and little non-functional ones."
  },
  {
    "objectID": "Exercises/Take-home_Ex1/Take-home_Ex1.html#section",
    "href": "Exercises/Take-home_Ex1/Take-home_Ex1.html#section",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good",
    "section": "6 ",
    "text": "6"
  },
  {
    "objectID": "Exercises/Take-home_Ex1/Take-home_Ex1.html#section-1",
    "href": "Exercises/Take-home_Ex1/Take-home_Ex1.html#section-1",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good",
    "section": "4 ",
    "text": "4 \n----\n\n4.0.1 Group water points according to their functional status at the LGA level\nNext, we will group the water points at the LGA level and according to their functional status.\n\n# wpdx_nigeria_group <- wpdx_nigeria_clean %>%\n#   group_by(`#clean_adm2`, `#status_clean`) %>%\n#   summarise(water_points = n()) %>%\n#   ungroup() \n\nWe will use pivot_wider() from tidyr package. This is used to pivot row values to columns. It “widens” data, increasing the number of columns and decreasing the number of rows\n\n# wpdx_nigeria_group <- wpdx_nigeria_group %>%\n#   pivot_wider(names_from = `#status_clean`, values_from = `water_points`)\n\nFinally, we can do a check using glimpse().\n\n# glimpse(wpdx_nigeria_clean)\n\nOur dataset has one column that specifies the LGA and the remaining six columns specifies the number of water points by its functional statuses."
  },
  {
    "objectID": "Exercises/Take-home_Ex1/Take-home_Ex1.html#global-spatial-autocorrelation",
    "href": "Exercises/Take-home_Ex1/Take-home_Ex1.html#global-spatial-autocorrelation",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good",
    "section": "5 Global Spatial Autocorrelation",
    "text": "5 Global Spatial Autocorrelation\nThis section will cover the computation of global spatial autocorrelation statistics and spatial complete randomness test for global spatial autocorrelation. The goal of these analyses is to understand whether water points are evenly distributed across Nigeria.\n\n5.1 Computing the Weight Matrix\nThere are two commonly used weight matrix: contiguity-based and distanced-based.\nRefer to this for\nhttps://towardsdatascience.com/what-is-exploratory-spatial-data-analysis-esda-335da79026ee\ncorerlogram : https://geodacenter.github.io/workbook/5a_global_auto/lab5a.html#spatial-correlogram\n----\n\n5.1.1 Group water points according to their functional status at the LGA level\nNext, we will group the water points at the LGA level and according to their functional status.\n\n# wpdx_nigeria_group <- wpdx_nigeria_clean %>%\n#   group_by(`#clean_adm2`, `#status_clean`) %>%\n#   summarise(water_points = n()) %>%\n#   ungroup() \n\nWe will use pivot_wider() from tidyr package. This is used to pivot row values to columns. It “widens” data, increasing the number of columns and decreasing the number of rows\n\n# wpdx_nigeria_group <- wpdx_nigeria_group %>%\n#   pivot_wider(names_from = `#status_clean`, values_from = `water_points`)\n\nFinally, we can do a check using glimpse().\n\n# glimpse(wpdx_nigeria_clean)\n\nOur dataset has one column that specifies the LGA and the remaining six columns specifies the number of water points by its functional statuses."
  },
  {
    "objectID": "Exercises/In-class_Ex2/In-class_Ex2.html",
    "href": "Exercises/In-class_Ex2/In-class_Ex2.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "In this exercise, we will be using the following packages:\n\nsf: for importing, managing, and processing geospatial data\ntmap: for performing data science tasks such as importing, wrangling and visualising data\ntidyverse: for performing data science tasks such as importing, wrangling and visualising data\nspdep: used to create spatial weights matrix objects from polygon ‘contiguities’\nfunModeling: used for rapid Exploratory Data Analysis\n\nWe will use pacman, a R package management tool to install and load the packages.\n\npacman::p_load(sf, tmap, tidyverse, spdep, funModeling)"
  },
  {
    "objectID": "Exercises/In-class_Ex2/In-class_Ex2.html#importing-geospatial-data",
    "href": "Exercises/In-class_Ex2/In-class_Ex2.html#importing-geospatial-data",
    "title": "In-class Exercise 2",
    "section": "2 Importing Geospatial Data",
    "text": "2 Importing Geospatial Data\nIn this in-class exercise, two geospatial data sets will be used:\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area or LGA) polygon feature GIS data. The data was obtained from geoBoundaries.\nWPdx+ data set that was obtained from Water Point Data Exchange (WPdx). It consists of water point related data from rural areas at the water point or small water scheme level. The entire set of data includes countries other than Nigeria. Hence, we will be performing data pre-processing to extract the relevant data. This is also in the shape file format.\n\n\n2.1 Import Nigeria LGA boundary data\n\nnga <- st_read(dsn = \"data\\\\geospatial\\\\\",\n                   layer = \"geodata_NGA_ADM2\",\n                   crs = 4326)\n\nReading layer `geodata_NGA_ADM2' from data source \n  `C:\\ameliachuayt\\ISSS624\\Exercises\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n2.2 Import Nigeria Water Points data\nThe code chunk below will import our geospatial data. We indicated crs = 4326 as we know that this data is in the wgs84 format. We know this by looking at the projection file (.prj contains projection information).\nWhile there are various file formats available for download e.g. .csv, .shp. The benefit of using the .shp file is that we can save one step of converting the .csv file into geospatial file using the latitude and longitude data.\n\nwp <- st_read(dsn = \"data\\\\geospatial\\\\\",\n                   layer = \"wp_geodata\",\n                   crs = 4326) %>% \n  filter(clean_coun == \"Nigeria\")\n\nNext, we will use write_rds() of readr package to save the extracted sf data table (i.e. wp) into an output file in .rds data format. The output file is called wp_nga.rds and it is saved in geospatial sub-folder.\n\nwp_nga <- write_rds(wp, \n                   \"data\\\\geospatial\\\\wp_nga.rds\")"
  },
  {
    "objectID": "Exercises/In-class_Ex2/In-class_Ex2.html#data-wrangling",
    "href": "Exercises/In-class_Ex2/In-class_Ex2.html#data-wrangling",
    "title": "In-class Exercise 2",
    "section": "3 Data Wrangling",
    "text": "3 Data Wrangling\n\n3.1 Recoding NGA values into string\nWe will read the saved .rsd file. It will look at the status_cle column and replace NA values with ‘unknown’ using replace_na().\n\nwp_nga <- read_rds('data\\\\geospatial\\\\wp_nga.rds') %>%\n  mutate(status_cle = replace_na(status_cle, \"Unknown\"))\n\n\n\n3.2 Exploratory Data Analysis\nIn the code chunk below, freq() of funModelling package is used to display the distribution of status_cle field in wp_nga.\n\nfreq(data = wp_nga,\n      input = 'status_cle')\n\nFrom the output above, we can see that there are 9 categories. Almost half of all water pipes are functional and approximately 11% with status unknown.\n\n\n3.3 Extract functional water points\nIn the code chunk below, filter() of dplyr package is used to select functional water points.\n\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nUsing the below code chunk, we can see the breakdown of functional statuses.\n\nfreq(data = wpt_functional, input = 'status_cle')\n\nClose to 80% of the water points are fully functional. The rest requires repair or are not in use.\n\n\n3.4 Extract non-functional water points\nWe can do the same for non-functional water points.\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Non-Functional\",\n             \"Non-Functional due to dry season\",\n             \"Non Functional due to dry season\",\n             \"Abandoned/Decommissioned\",\n             \"Abandoned\"))\n\nUsing the below code chunk, we can see the breakdown of non-functional statuses.\n\nfreq(data = wpt_nonfunctional, input = 'status_cle')\n\nThere is 90% of water points that are non-functional, another 7% are non-functional due to the weather. The remaining are abandoned or decommissioned.\n\n\n3.5 Extract water point with unknown class\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")\n\n\n\n3.6 Performing Point-in-Polygon count\nIn the code chunk below, we will use st_intersect() yo identify which LGA respectively types of water points are located in and lengths() to count the number of water points that fall inside each LGA area.\n\nnga_wp <- nga %>%\n  mutate(`total_wpt` = lengths(\n    st_intersects(nga, wp_nga))) %>%\n  mutate(`wpt_functional` = lengths(\n    st_intersects(nga, wpt_functional))) %>%\n  mutate(`wpt_nonfunctional` = lengths(\n    st_intersects(nga, wpt_nonfunctional))) %>%\n  mutate(`wpt_unknown` = lengths(\n    st_intersects(nga, wpt_unknown)))"
  },
  {
    "objectID": "Exercises/In-class_Ex2/In-class_Ex2.html#saving-the-analytical-data-table",
    "href": "Exercises/In-class_Ex2/In-class_Ex2.html#saving-the-analytical-data-table",
    "title": "In-class Exercise 2",
    "section": "4 Saving the Analytical Data Table",
    "text": "4 Saving the Analytical Data Table\n\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = `wpt_functional`/ `total_wpt`) %>%\n  mutate(pct_nonfunctional = `wpt_nonfunctional`/ `total_wpt`) %>%\n  select(1, 6:10)\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr package is used to derive two fields namely pct_functional and pct_non-functional.\nto keep the file size small, select() of dplyr is used to retain selected fields\n\nLet’s save the new sf data table into .rds format.\n\nwrite_rds(nga_wp,\n          \"data\\\\geospatial\\\\wp_nga.rds\")"
  },
  {
    "objectID": "Exercises/In-class_Ex2/In-class_Ex2.html#visualising-the-spatial-distribution-of-water-points",
    "href": "Exercises/In-class_Ex2/In-class_Ex2.html#visualising-the-spatial-distribution-of-water-points",
    "title": "In-class Exercise 2",
    "section": "5 Visualising the Spatial Distribution of Water Points",
    "text": "5 Visualising the Spatial Distribution of Water Points\nIn the code chuck below, we will use qtm() to quickly plot the spatial distribution of water points of different categories. We will use tmap_arrange() to show the plots together.\n\n\n\n\ntotal <- qtm(nga_wp, fill = 'total_wpt')\nfunc <- qtm(nga_wp, fill = 'wpt_functional')\nnonfunc <- qtm(nga_wp, fill = 'wpt_nonfunctional')\nunknown <- qtm(nga_wp, fill = 'wpt_unknown')\n\ntmap_arrange(total, unknown, func, nonfunc, asp = 1, ncol = 2)\n\n\n\n\nFrom the maps above, we can see that there are some areas with little (<200) water points and areas with >800 water points. Also, towards the southern area of the map, we can see that there are areas where water pumps statuses are unknown, probably due to lack of updating. It appears that the top center of Nigeria have higher amount of functional water points."
  },
  {
    "objectID": "Exercises/In-class_Ex2/In-class_Ex2.html#conclusion",
    "href": "Exercises/In-class_Ex2/In-class_Ex2.html#conclusion",
    "title": "In-class Exercise 2",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nFrom what we have achieved in this exercise, we have grounds to believe that there might exist some relationship in how water points co-occur in Nigeria. We will explore more in Take-Home Exercise 1."
  }
]